One could imagine trying to sample triangulations according to \eqref{eq:part_sum}.
However, this is not very practical. It turns out that $\Omega(n) \sim n! 2^n$. This means that for $\lambda > \ln 2$ large volumes are suppressed so that a typical triangulation will contain only a handful of triangles.
However, when $\lambda < \ln 2$ the partition sum \eqref{eq:part_sum} diverges and the problem is ill-defined.

A solution is to consider only triangulations of a certain fixed volume at a time.
An advantage of this approach is that the desired distribution becomes uniform, since the weight of each triangulation depends only on its volume.
In practice, this can be obtained by using update rules that keep the number of triangles fixed.

\subsection{Update rules}
% Should we also include alternative update rules we attempted? And why the don't work? It does show the amount of work and consideration we put into constcuting an effective MCMC simulation, but Im not sure.

\begin{figure}
    \centering
    \begin{tikzpicture}

        % initial state for source
        % timeslices
        \draw (0, 0) -- (4, 0);
        \draw (0, 1) -- (4, 1);
        \draw (0, 2) -- (4, 2);

        % timelike connections
        \draw (0, 0) -- (1, 1) -- (0, 2);
        \draw (2, 0) -- (1, 1) -- (2, 2);
        \draw (2, 0) -- (3, 1) -- (2, 2);
        \draw (4, 0) -- (3, 1) -- (4, 2);

        % shard
        \draw[triangle = {red}] (2, 0) -- (3, 1) -- (2, 1) -- cycle;
        \draw[triangle = {red}] (2, 1) -- (3, 1) -- (2, 2) -- cycle;

        % order 4 vertex
        \node[order4] at (2, 1){};

        % initial state for destination
        % timeslices
        \draw (0, 3) -- (4, 3);
        \draw (0, 4) -- (4, 4);
        \draw (0, 5) -- (4, 5);

        % timelike connections
        \draw (0, 3) -- (1, 4) -- (0, 5);
        \draw (2, 3) -- (1, 4) -- (2, 5);
        \draw (2, 3) -- (3, 4) -- (2, 5);
        \draw (4, 3) -- (3, 4) -- (4, 5);

        % crack
        \draw[edge = {blue}] (2, 3) -- (3, 4) -- (2, 5);

        % final state for source
        % timeslices
        \draw (6, 0) -- (10, 0);
        \draw (6, 1) -- (10, 1);
        \draw (6, 2) -- (10, 2);

        % timelike connections
        \draw (6, 0) -- (7, 1) -- (6, 2);
        \draw (8, 0) -- (7, 1) -- (8, 2);
        \draw (8, 0) -- (9, 1) -- (8, 2);
        \draw (10, 0) -- (9, 1) -- (10, 2);

        % crack
        \draw[edge = {red}] (8, 0) -- (9, 1) -- (8, 2);

        % final state for destination
        % timeslices
        \draw (6, 3) -- (10, 3);
        \draw (6, 4) -- (10, 4);
        \draw (6, 5) -- (10, 5);

        % timelike connections
        \draw (6, 3) -- (7, 4) -- (6, 5);
        \draw (8, 3) -- (7, 4) -- (8, 5);
        \draw (8, 3) -- (9, 4) -- (8, 5);
        \draw (10, 3) -- (9, 4) -- (10, 5);

        % shard
        \draw[triangle = {blue}] (8, 3) -- (9, 4) -- (8, 4) -- cycle;
        \draw[triangle = {blue}] (8, 4) -- (9, 4) -- (8, 5) -- cycle;

        % order 4 vertex
        \node[order4] at (8, 4){};

        % arrow
        \draw[->, ultra thick] (4.5, 2.5) -- (5.5, 2.5);

    \end{tikzpicture}
    \caption{Shard move update rule. Shard source in red (bottom), destination in blue (top). Initial (left) and final (right) states. Order 4 vertices affected by this move are marked with a green dot.}
    \label{fig:shard_move}
\end{figure}

\begin{figure}
    \centering
    \begin{tikzpicture}
        % initial state
        % timeslices
        \draw (0, 0) -- (3, 0);
        \draw (0, 1) -- (3, 1);

        % timelijke connections
        \draw (1, 0) -- (0, 1);
        \draw (1, 0) -- (1, 1);
        \draw (2, 0) -- (2, 1);
        \draw (2, 0) -- (3, 1);

        % diagonal
        \draw[edge = {blue}] (1, 0) -- (2, 1);

        % maybe order 4 vertices
        \node[maybe_order4] at (1, 1){};
        \node[maybe_order4] at (2, 0){};

        % final state
        % timeslices
        \draw (5, 0) -- (8, 0);
        \draw (5, 1) -- (8, 1);

        % timelijke connections
        \draw (6, 0) -- (5, 1);
        \draw (6, 0) -- (6, 1);
        \draw (7, 0) -- (7, 1);
        \draw (7, 0) -- (8, 1);

        % diagonal
        \draw[edge = {blue}] (6, 1) -- (7, 0);

        % maybe order 4 vertices
        \node[maybe_order4] at (6, 0){};
        \node[maybe_order4] at (7, 1){};

        % arrow
        \draw[<->, ultra thick] (3.5, 0.5) -- (4.5, 0.5);
    \end{tikzpicture}
    \caption{Flip move update rule. The flipped edge is shown in blue. Order 4 vertices (if they exist) are marked with a green circle.}
    \label{fig:flip_move}
\end{figure}

\subsection{Observables} \label{sec:observables}
% Explain choice of observables, possibly alternatives that were considered but not measured
% Maybe also explain how lengthprofile is obtained from implementation, as this is not entirely trivial and Timothy asked about this after the presentation
Finding good observables to measure is quite challenging in (C)DT. This may partly be due to a lack of experimental data to compare simulations to.
But also where many Monte Carlo simulations in physics are concerned with describing the behaviour of fields on a lattice, naturally giving some combination of the field values as observables,
there is no such field in CDT but the lattice itself should provide the information of interest.

The chosen observables can of course not depend on any details of the implementation, like the use of labelling in our implementation.
And for an observable to be of physical interest it should also have a well-defined limit for the amount of triangles $N \rightarrow \infty$.
Observables that are often looked at are different notions of dimension, notably the \emph{spectral dimension} \cite{2012} and \emph{Hausdorff dimension} \cite{1998, 2012}, the latter of which has also been discussed in the case of Dynamical Triangulation in the course Monte Carlo Techniques. And a newer observable of interest is a certain notion of \emph{Quantum Curvature} -- a sort of extended notion of Ricci curvature which can be applied to the used triangulations and obeys a proper limit \cite{brunekreef2021}.

However since the focus of this project was more in the implementation of a Markov Chain Monte Carlo simulation, and the measurements and analysis of these observables is somewhat involved, we only consider observables that are derived from the volume of the space-like slices.
This means that in $1 + 1$D CDT we look at the \emph{length}, that is the amount of edges or equivalently the amount of vertices in a time-slice, at different times; this we call the \emph{length profile} $\ell(t)$

\paragraph{Length profile}
The length profile itself still depends on a certain choice of origin in $t$, while we consider a toroidal topology so any there is no preferred origin. Thus, any observable should have some sort of averaging over $t$.

The simplest observable to consider is the average of $\ell(t)$, but this is trivially $L$ since the total amount timeslices and triangles and thus vertices is kept fixed.
So the simplest non-trivial observable we can think of is the standard deviation $\sigma_\ell$ of the length profile $\ell(t)$:
\begin{equation}
    \sigma_\ell^2 = \frac{1}{T} \sum_{t = 1}^{T} \Big(\ell(t) - L\Big)^2,
\end{equation}
using the usual definition of the population variance.

%TODO possibly we want to use a different notion of length correlation
But $\sigma_\ell$ does not contain any information of the relation of lengths between different times $t$.
So an arguably more interesting observable is what we call the \emph{length correlation}:
\begin{equation}
    \rho_\ell(t) = \frac{1}{T \sigma_\ell^2} \sum_{t_0 = 1}^{T} \qty(\ell(t_0) - L)\qty(\ell(t_0 + t) - L),
\end{equation}
where this correlation is normalised such that $\rho_\ell(0) = 1$.
Note that the length correlation is still a function of $t$ but this $t$ is the time difference between two time-slices, so there is no origin dependence.

Both observables $\sigma_\ell$ and $\rho_\ell(t)$ do diverge for $N \rightarrow \infty$ as they will scale with $N$.
How they will scale is at this point not known, but we hope they will scale in a well-behaved way such that with proper rescaling these observables may have a well-defined limit.

\subsection{Implementation}
% This was probably the most substantional amount of work of this project, so I think this should be reflected in the size of this part
% Also here I'm not sure whether to include the failed attempts?

\paragraph{Length profile}
% Explain how length profile was obtained from implementation